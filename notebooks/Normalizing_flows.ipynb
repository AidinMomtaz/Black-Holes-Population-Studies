{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing flows are for powerful distribution approximation.\n",
    "\n",
    "For more detail, [this](https://lilianweng.github.io/posts/2018-10-13-flow-models/) and especially [this](https://blog.evjang.com/2018/01/nf1.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing flows are for more powerful distribution approximation.\n",
    "The name “normalizing flow” can be interpreted as the following:\n",
    "\n",
    "1. “Normalizing” means that the change of variables gives a normalized density after applying an invertible transformation.\n",
    "2. “Flow” means that the invertible transformations can be composed with each other to create more complex invertible transformations.\n",
    "\n",
    "In normalizing flows, we wish to map simple distributions (easy to sample and evaluate densities) to complex ones (learned via data)by applying a sequence of invertible transformation functions. Flowing through a chain of transformations, we repeatedly substitute the variable for the new one according to the change of variables theorem and eventually obtain a probability distribution of the final target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notebook-images/normalizing-flow.png\" alt=\"Alt Text\" width=\"900\"/>\n",
    "\n",
    "\n",
    "*Figure 1: A representation of normalizing flows, illustrating the transformation of a simple, easy-to-sample distribution into a complex target distribution. This is achieved by applying a sequence of invertible transformations, where each step leverages the change of variables theorem to update the density of the transformed variable progressively toward the desired target distribution.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Change of Variables Formula\n",
    "\n",
    "The change of variables formula describes how to evaluate the densities of a random variable that is a deterministic transformation from another variable.\n",
    "**Change of Variables:** \n",
    "\n",
    "**Change of Variables:** Let _Z_ and _X_ be random variables that are related by a mapping  \n",
    "$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ such that $X = f(Z)$ and $Z = f^{-1}(X)$. Then:\n",
    "\n",
    "$$\n",
    "p_X(\\mathbf{x}) = p_Z\\left(f^{-1}(\\mathbf{x})\\right) \\left|\\det\\left(\\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|\n",
    "$$\n",
    "\n",
    "*  The input and output dimensions must be the same.\n",
    "*  The transformation must be invertible.\n",
    "*  Computing the determinant of the Jacobian needs to be efficient and differentiable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Golden Quote\n",
    "**Determinants are nothing more than the amount (and direction) of volume distortion of a linear transformation, generalized to any number of dimensions or the local, linearized rate of volume change of a transformation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training criterion\n",
    "\n",
    "With normalizing flows in our toolbox, the exact log-likelihood of input data, $\\log p(\\mathbf{x})$ becomes tractable. As a result, the training criterion of flow-based generative model is simply the negative log-likelihood (NLL) over the training dataset $\\mathcal{D}$.\n",
    "$$\\mathcal{L}(\\mathcal{D}) = - \\frac{1}{\\vert\\mathcal{D}\\vert}\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models of Normalizing Flows\n",
    " [RealNVP](https://arxiv.org/abs/1605.08803), [Glow](https://arxiv.org/abs/1807.03039) and [MADE](https://arxiv.org/abs/1502.03509).\n",
    "  \n",
    "TensorFlow has a nice [set of functions](https://arxiv.org/pdf/1711.10604) that make it easy to build flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed Distributions in TensorFlow\n",
    "TensorFlow has an elegant API for transforming distributions. A `TransformedDistribution` is specified by a **base distribution** object that we will transform, and a **Bijector** object that implements:\n",
    "\n",
    "1. A forward transformation $y = f(x)$, where $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "2. Its inverse transformation $x = f^{-1}(y)$\n",
    "3. The inverse log determinant of the Jacobian $\\log |\\det J(f^{-1}(y))|$\n",
    "\n",
    "For the rest of this post, I will abbreviate this quantity as **ILDJ**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this abstraction, forward sampling is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijector.forward(base_dist.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate log-density of the transformed distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution.log_prob(bijector.inverse(x)) + bijector.inverse_log_det_jacobian(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic Normalizing Flow in TensorFlow\n",
    "\n",
    "TF [Distributions](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution) - general API for manipulating distributions in TF.\n",
    "\n",
    "TF [Bijector](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors) - general API for creating operators on distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to model the distribution $p(x_1, x_2) = \\mathcal{N}(x_1 \\mid \\mu = \\frac{1}{4} x_2^2, \\sigma = 1) \\cdot \\mathcal{N}(x_2 \\mid \\mu = 0, \\sigma = 4)$. We can generate samples from the target distribution using the following code snippet (we generate them in TensorFlow to avoid having to copy samples from the CPU to the GPU on each minibatch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "x2_dist = tfd.Normal(loc=0., scale=4.)\n",
    "x2_samples = x2_dist.sample(batch_size)\n",
    "x1 = tfd.Normal(loc=.25 * tf.square(x2_samples),\n",
    "                scale=tf.ones(batch_size, dtype=tf.float32))\n",
    "x1_samples = x1.sample()\n",
    "x_samples = tf.stack([x1_samples, x2_samples], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
